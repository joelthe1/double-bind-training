{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Colin's version of train_lm.ipynb from https://github.com/krypticmouse/double-bind-training/commit/9f0351b7c162a48175b07ae717cb6680517e488f"
      ],
      "metadata": {
        "id": "DbmKjYRj6OM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyMJ-uU8tzj0",
        "outputId": "d0837eb7-0fb6-4f6a-f2f5-4517c7e2f823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'double-bind-training'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 49 (delta 19), reused 35 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (49/49), 508.71 KiB | 3.46 MiB/s, done.\n",
            "/content/double-bind-training\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/krypticmouse/double-bind-training.git\n",
        "%cd double-bind-training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout train-lm-adapter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgtGux6Lt7PW",
        "outputId": "5004b042-fb1b-4490-90f8-72859b36b192"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'train-lm-adapter' set up to track remote branch 'train-lm-adapter' from 'origin'.\n",
            "Switched to a new branch 'train-lm-adapter'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install adapter-transformers seqeval ptvsd wandb datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jTFochFt9ld",
        "outputId": "136f762a-5fc2-4ba5-e231-d8fb2b9a1d14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting adapter-transformers\n",
            "  Downloading adapter_transformers-3.1.0-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ptvsd\n",
            "  Downloading ptvsd-4.3.2-py2.py3-none-any.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.13.9-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from adapter-transformers) (3.9.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.0.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.14.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.4.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->adapter-transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->adapter-transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->adapter-transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->adapter-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->adapter-transformers) (2.10)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: seqeval, pathtools\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=56443d54a2a0f01194a7e060eb676e72c028ad2f70520545f417b1b0ad12c06b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=1844c7d98e8a1023f4510f926ba6edabccd92ffb7b9b2a11c571441934e63d8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built seqeval pathtools\n",
            "Installing collected packages: tokenizers, pathtools, xxhash, urllib3, smmap, setproctitle, ptvsd, multiprocess, docker-pycreds, sentry-sdk, gitdb, seqeval, responses, huggingface-hub, GitPython, wandb, datasets, adapter-transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed GitPython-3.1.30 adapter-transformers-3.1.0 datasets-2.9.0 docker-pycreds-0.4.0 gitdb-4.0.10 huggingface-hub-0.12.0 multiprocess-0.70.14 pathtools-0.1.2 ptvsd-4.3.2 responses-0.18.0 sentry-sdk-1.14.0 seqeval-1.2.2 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.12.1 urllib3-1.26.14 wandb-0.13.9 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "YVt-1YPlt_S5",
        "outputId": "9e717f55-6648-4a21-e3c0-af6139081098"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download and preprocess training data from MAFAND\n",
        "https://huggingface.co/datasets/masakhane/mafand is the Mafand dataset. It is intended for machine translation but we can just use it for language modeling. \n",
        "\n",
        "['en-amh', 'en-hau', 'en-ibo', 'en-kin', 'en-lug', 'en-nya', 'en-pcm', 'en-sna', 'en-swa', 'en-tsn', 'en-twi', 'en-xho', 'en-yor', 'en-zul', 'fr-bam', 'fr-bbj', 'fr-ewe', 'fr-fon', 'fr-mos', 'fr-wol'] are the available translation sets. "
      ],
      "metadata": {
        "id": "pYx6Vo3p2jUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"masakhane/mafand\", \"en-wol\")"
      ],
      "metadata": {
        "id": "3xSuRdcKHD-C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "604bfaac-415a-46b1-b39c-1cf69b233968"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-b03fe4ce18b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"masakhane/mafand\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en-wol\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1736\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m     \u001b[0;31m# Instantiate the dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m     builder_instance: DatasetBuilder = builder_cls(\n\u001b[0m\u001b[1;32m   1520\u001b[0m         \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m         \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, writer_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m         \u001b[0;31m# Batch size used by the ArrowWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# It defines the number of samples that are kept in memory before writing them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_dir\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         self.config, self.config_id = self._create_builder_config(\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mcustom_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mbuilder_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuilder_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBUILDER_CONFIGS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    463\u001b[0m                     \u001b[0;34mf\"BuilderConfig {config_name} not found. Available: {list(self.builder_configs.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: BuilderConfig en-wol not found. Available: ['en-amh', 'en-hau', 'en-ibo', 'en-kin', 'en-lug', 'en-nya', 'en-pcm', 'en-sna', 'en-swa', 'en-tsn', 'en-twi', 'en-xho', 'en-yor', 'en-zul', 'fr-bam', 'fr-bbj', 'fr-ewe', 'fr-fon', 'fr-mos', 'fr-wol']"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (implemented) Convert translation to language modeling set\n",
        "\n",
        "Pull out one language, convert to language modeling set. Language modeling sets are monolingual, and each data item has a \"text\" field like this example from https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/01_Adapter_Training.ipynb\n",
        "\n",
        "```\n",
        "dataset = load_dataset(\"rotten_tomatoes\")\n",
        "dataset['train'][0]\n",
        "{'label': 1,\n",
        " 'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n",
        "```\n",
        "\n",
        "We should be able to just pull out all the ones with one language from \"train\", and all the ones from \"validation\" and all the ones from \"test\", then reformat them to be formatted like that. "
      ],
      "metadata": {
        "id": "Bw5CE_8g4sS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flat_dataset = dataset.flatten()\n",
        "flat_dataset"
      ],
      "metadata": {
        "id": "wvyVFKI-42pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8578e52c-1c0a-43aa-e3dc-d7053829d7c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation.en', 'translation.swa'],\n",
              "        num_rows: 30782\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['translation.en', 'translation.swa'],\n",
              "        num_rows: 1791\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation.en', 'translation.swa'],\n",
              "        num_rows: 1835\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "language_code = \"swa\"\n",
        "\n",
        "for split in flat_dataset:\n",
        "  flat_dataset[split] = flat_dataset[split].remove_columns('translation.en')"
      ],
      "metadata": {
        "id": "H_p7NnTR2x1x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in flat_dataset:\n",
        "  split_strings = []\n",
        "  for data_item in flat_dataset[split]:\n",
        "    values = data_item.values()\n",
        "    for value in values: \n",
        "      # print(value)\n",
        "      split_strings.append(value)\n",
        "  with open(f\"{split}.txt\", \"w\") as spf:\n",
        "    spf.writelines('\\n'.join(split_strings))"
      ],
      "metadata": {
        "id": "VYjVwBYUHWVr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBcnREns4d5M",
        "outputId": "cb893315-10b2-40f7-d1ea-60777b35d37e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/double-bind-training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /tmp/test-mlm"
      ],
      "metadata": {
        "id": "-sWHINZJYQTX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_lm_adapter.py \\\n",
        "    --model_name_or_path roberta-base \\\n",
        "    --train_file train.txt \\\n",
        "    --validation_file validation.txt \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --train_adapter \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --report_to \"wandb\" \\\n",
        "    --run_name \"adapter-training-lm-test\" \\\n",
        "    --output_dir /tmp/test-mlm \\\n",
        "    --dataset_language \"swa\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvAFw4rFLC51",
        "outputId": "6bcd1c33-c6f7-41a5-a0d3-85bd5db2b862"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkrypticmouse\u001b[0m (\u001b[33mdouble-bind-ner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/double-bind-training/wandb/run-20230127_084732-uvj1qh4w\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlegendary-rabbit-8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/double-bind-ner/training-lm-test-run\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/double-bind-ner/training-lm-test-run/runs/uvj1qh4w\u001b[0m\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/test-mlm/runs/Jan27_08-47-32_ea99d13d1d7e,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/tmp/test-mlm,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=adapter-training-lm-test,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-a65a4c68c4ce444b\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/text\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/text/default-a65a4c68c4ce444b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-a65a4c68c4ce444b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/text/default-a65a4c68c4ce444b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "100% 2/2 [00:00<00:00, 715.75it/s]\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 08:47:35,413 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 08:47:35,414 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:404] 2023-01-27 08:47:36,327 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 08:47:37,239 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 08:47:37,241 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 08:47:43,587 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 08:47:43,588 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 08:47:43,588 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 08:47:43,588 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 08:47:43,589 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2023-01-27 08:47:43,589 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:681] 2023-01-27 08:47:44,501 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
            "[INFO|configuration_utils.py:730] 2023-01-27 08:47:44,502 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.21.3\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2041] 2023-01-27 08:47:45,478 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
            "[INFO|modeling_utils.py:2435] 2023-01-27 08:47:47,170 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "[INFO|modeling_utils.py:2443] 2023-01-27 08:47:47,170 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
            "[INFO|configuration.py:725] 2023-01-27 08:47:47,177 >> Adding adapter 'mlm'.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-a65a4c68c4ce444b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-28b970e51204ded1.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-a65a4c68c4ce444b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-4313f221dcf34d9b.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-a65a4c68c4ce444b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-7c20d408c49c8eaf.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-a65a4c68c4ce444b/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f80e632e78d3bc69.arrow\n",
            "train_lm_adapter.py:581: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"accuracy\")\n",
            "[INFO|trainer.py:722] 2023-01-27 08:47:50,527 >> The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1605] 2023-01-27 08:47:50,537 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2023-01-27 08:47:50,537 >>   Num examples = 3178\n",
            "[INFO|trainer.py:1607] 2023-01-27 08:47:50,538 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1608] 2023-01-27 08:47:50,538 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1609] 2023-01-27 08:47:50,538 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1610] 2023-01-27 08:47:50,538 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2023-01-27 08:47:50,538 >>   Total optimization steps = 398\n",
            "[INFO|integrations.py:607] 2023-01-27 08:47:50,540 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "100% 398/398 [05:25<00:00,  1.56it/s][INFO|trainer.py:1850] 2023-01-27 08:53:15,835 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 325.2963, 'train_samples_per_second': 9.77, 'train_steps_per_second': 1.223, 'train_loss': 3.2486569198531723, 'epoch': 1.0}\n",
            "100% 398/398 [05:25<00:00,  1.22it/s]\n",
            "[INFO|loading.py:60] 2023-01-27 08:53:15,838 >> Configuration saved in /tmp/test-mlm/adapter_config.json\n",
            "[INFO|loading.py:73] 2023-01-27 08:53:15,851 >> Module weights saved in /tmp/test-mlm/pytorch_adapter.bin\n",
            "[INFO|loading.py:60] 2023-01-27 08:53:15,852 >> Configuration saved in /tmp/test-mlm/head_config.json\n",
            "[INFO|loading.py:73] 2023-01-27 08:53:16,194 >> Module weights saved in /tmp/test-mlm/pytorch_model_head.bin\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     3.2487\n",
            "  train_runtime            = 0:05:25.29\n",
            "  train_samples            =       3178\n",
            "  train_samples_per_second =       9.77\n",
            "  train_steps_per_second   =      1.223\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:722] 2023-01-27 08:53:16,198 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2891] 2023-01-27 08:53:16,200 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2023-01-27 08:53:16,200 >>   Num examples = 189\n",
            "[INFO|trainer.py:2896] 2023-01-27 08:53:16,200 >>   Batch size = 8\n",
            "100% 24/24 [00:09<00:00,  2.82it/s]INFO:datasets.metric:Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
            "100% 24/24 [00:09<00:00,  2.64it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.4635\n",
            "  eval_loss               =     2.9812\n",
            "  eval_runtime            = 0:00:09.47\n",
            "  eval_samples            =        189\n",
            "  eval_samples_per_second =     19.949\n",
            "  eval_steps_per_second   =      2.533\n",
            "  perplexity              =    19.7122\n",
            "[INFO|modelcard.py:467] 2023-01-27 08:53:26,590 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.46351683282762607}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh /tmp/test-mlm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A79V3tzHXvKL",
        "outputId": "8f516c32-1026-466d-ca3c-e989e3a3c971"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 154M\n",
            "drwxr-xr-x 2 root root 4.0K Jan 27 08:53 .\n",
            "drwxrwxrwt 1 root root 4.0K Jan 27 08:53 ..\n",
            "-rw-r--r-- 1 root root 1022 Jan 27 08:53 adapter_config.json\n",
            "-rw-r--r-- 1 root root  434 Jan 27 08:53 all_results.json\n",
            "-rw-r--r-- 1 root root  263 Jan 27 08:53 eval_results.json\n",
            "-rw-r--r-- 1 root root  253 Jan 27 08:53 head_config.json\n",
            "-rw-r--r-- 1 root root 3.5M Jan 27 08:53 pytorch_adapter.bin\n",
            "-rw-r--r-- 1 root root 150M Jan 27 08:53 pytorch_model_head.bin\n",
            "-rw-r--r-- 1 root root 1.1K Jan 27 08:53 README.md\n",
            "-rw-r--r-- 1 root root  588 Jan 27 08:53 trainer_state.json\n",
            "-rw-r--r-- 1 root root  191 Jan 27 08:53 train_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env MAX_LENGTH=164\n",
        "%env ADAPTER_MODEL=roberta-base\n",
        "%env OUTPUT_DIR=swa_sample\n",
        "%env BATCH_SIZE=32\n",
        "%env NUM_EPOCHS=50\n",
        "%env SAVE_STEPS=10000\n",
        "%env SEED=1\n",
        "\n",
        "! CUDA_VISIBLE_DEVICES=1 python3 train_ner_adapter.py --data_dir data/wol/ \\\n",
        "--model_type bert \\\n",
        "--model_name_or_path $ADAPTER_MODEL \\\n",
        "--output_dir $OUTPUT_DIR \\\n",
        "--max_seq_length  $MAX_LENGTH \\\n",
        "--num_train_epochs $NUM_EPOCHS \\\n",
        "--per_gpu_train_batch_size $BATCH_SIZE \\\n",
        "--save_steps $SAVE_STEPS --learning_rate 5e-4 \\\n",
        "--seed $SEED \\\n",
        "--dataset_language \"ner-swa\" \\\n",
        "--path_to_adapter /tmp/test-mlm \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13mHdAYnenlk",
        "outputId": "458f8b78-cd2f-425b-e12d-f0f2e18c2bef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: MAX_LENGTH=164\n",
            "env: ADAPTER_MODEL=roberta-base\n",
            "env: OUTPUT_DIR=swa_sample\n",
            "env: BATCH_SIZE=32\n",
            "env: NUM_EPOCHS=50\n",
            "env: SAVE_STEPS=10000\n",
            "env: SEED=1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkrypticmouse\u001b[0m (\u001b[33mdouble-bind-ner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/double-bind-training/wandb/run-20230127_090950-txt3oa4u\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchromatic-moon-57\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/double-bind-ner/masakhane-ner-test-run\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/double-bind-ner/masakhane-ner-test-run/runs/txt3oa4u\u001b[0m\n",
            "['O', 'B-DATE', 'I-DATE', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples =  1871\n",
            "  Num Epochs =  50.0\n",
            "  Instantaneous batch size per GPU =  32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) =  32\n",
            "  Gradient Accumulation steps =  1\n",
            "  Total optimization steps =  2950.0\n",
            "Epoch:   0% 0/50 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/59 [00:20<?, ?it/s]\n",
            "Epoch:   0% 0/50 [00:20<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"train_ner_adapter.py\", line 732, in <module>\n",
            "    main()\n",
            "  File \"train_ner_adapter.py\", line 631, in main\n",
            "    global_step, tr_loss = train(args, train_dataset, model, tokenizer, labels, pad_token_label_id, adapter_name)\n",
            "  File \"train_ner_adapter.py\", line 177, in train\n",
            "    loss = loss_fct(active_logits, active_labels)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
            "    return F.cross_entropy(input, target, weight=self.weight,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 3026, in cross_entropy\n",
            "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
            "ValueError: Expected input batch_size (29310080) to match target batch_size (5248).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QRJgs7xVp3a",
        "outputId": "65929262-b648-4409-976a-fd3221190d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/"
      ],
      "metadata": {
        "id": "0AWL5oXUYUTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -rv /tmp/test-mlm/* /gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5alLUwX5VySY",
        "outputId": "4696b686-cdba-46f0-b247-9cf3114ca923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/tmp/test-mlm/all_results.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/all_results.json'\n",
            "'/tmp/test-mlm/checkpoint-1000' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000'\n",
            "'/tmp/test-mlm/checkpoint-1000/mlm' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/mlm'\n",
            "'/tmp/test-mlm/checkpoint-1000/mlm/adapter_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/mlm/adapter_config.json'\n",
            "'/tmp/test-mlm/checkpoint-1000/mlm/pytorch_adapter.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/mlm/pytorch_adapter.bin'\n",
            "'/tmp/test-mlm/checkpoint-1000/mlm/head_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/mlm/head_config.json'\n",
            "'/tmp/test-mlm/checkpoint-1000/mlm/pytorch_model_head.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/mlm/pytorch_model_head.bin'\n",
            "'/tmp/test-mlm/checkpoint-1000/tokenizer_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/tokenizer_config.json'\n",
            "'/tmp/test-mlm/checkpoint-1000/special_tokens_map.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/special_tokens_map.json'\n",
            "'/tmp/test-mlm/checkpoint-1000/vocab.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/vocab.json'\n",
            "'/tmp/test-mlm/checkpoint-1000/merges.txt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/merges.txt'\n",
            "'/tmp/test-mlm/checkpoint-1000/tokenizer.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/tokenizer.json'\n",
            "'/tmp/test-mlm/checkpoint-1000/training_args.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/training_args.bin'\n",
            "'/tmp/test-mlm/checkpoint-1000/optimizer.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/optimizer.pt'\n",
            "'/tmp/test-mlm/checkpoint-1000/scheduler.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/scheduler.pt'\n",
            "'/tmp/test-mlm/checkpoint-1000/trainer_state.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/trainer_state.json'\n",
            "'/tmp/test-mlm/checkpoint-1000/rng_state.pth' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1000/rng_state.pth'\n",
            "'/tmp/test-mlm/checkpoint-1500' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500'\n",
            "'/tmp/test-mlm/checkpoint-1500/mlm' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/mlm'\n",
            "'/tmp/test-mlm/checkpoint-1500/mlm/adapter_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/mlm/adapter_config.json'\n",
            "'/tmp/test-mlm/checkpoint-1500/mlm/pytorch_adapter.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/mlm/pytorch_adapter.bin'\n",
            "'/tmp/test-mlm/checkpoint-1500/mlm/head_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/mlm/head_config.json'\n",
            "'/tmp/test-mlm/checkpoint-1500/mlm/pytorch_model_head.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/mlm/pytorch_model_head.bin'\n",
            "'/tmp/test-mlm/checkpoint-1500/tokenizer_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/tokenizer_config.json'\n",
            "'/tmp/test-mlm/checkpoint-1500/special_tokens_map.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/special_tokens_map.json'\n",
            "'/tmp/test-mlm/checkpoint-1500/vocab.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/vocab.json'\n",
            "'/tmp/test-mlm/checkpoint-1500/merges.txt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/merges.txt'\n",
            "'/tmp/test-mlm/checkpoint-1500/tokenizer.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/tokenizer.json'\n",
            "'/tmp/test-mlm/checkpoint-1500/training_args.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/training_args.bin'\n",
            "'/tmp/test-mlm/checkpoint-1500/optimizer.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/optimizer.pt'\n",
            "'/tmp/test-mlm/checkpoint-1500/scheduler.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/scheduler.pt'\n",
            "'/tmp/test-mlm/checkpoint-1500/trainer_state.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/trainer_state.json'\n",
            "'/tmp/test-mlm/checkpoint-1500/rng_state.pth' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-1500/rng_state.pth'\n",
            "'/tmp/test-mlm/checkpoint-2000' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000'\n",
            "'/tmp/test-mlm/checkpoint-2000/mlm' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/mlm'\n",
            "'/tmp/test-mlm/checkpoint-2000/mlm/adapter_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/mlm/adapter_config.json'\n",
            "'/tmp/test-mlm/checkpoint-2000/mlm/pytorch_adapter.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/mlm/pytorch_adapter.bin'\n",
            "'/tmp/test-mlm/checkpoint-2000/mlm/head_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/mlm/head_config.json'\n",
            "'/tmp/test-mlm/checkpoint-2000/mlm/pytorch_model_head.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/mlm/pytorch_model_head.bin'\n",
            "'/tmp/test-mlm/checkpoint-2000/tokenizer_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/tokenizer_config.json'\n",
            "'/tmp/test-mlm/checkpoint-2000/special_tokens_map.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/special_tokens_map.json'\n",
            "'/tmp/test-mlm/checkpoint-2000/vocab.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/vocab.json'\n",
            "'/tmp/test-mlm/checkpoint-2000/merges.txt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/merges.txt'\n",
            "'/tmp/test-mlm/checkpoint-2000/tokenizer.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/tokenizer.json'\n",
            "'/tmp/test-mlm/checkpoint-2000/training_args.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/training_args.bin'\n",
            "'/tmp/test-mlm/checkpoint-2000/optimizer.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/optimizer.pt'\n",
            "'/tmp/test-mlm/checkpoint-2000/scheduler.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/scheduler.pt'\n",
            "'/tmp/test-mlm/checkpoint-2000/trainer_state.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/trainer_state.json'\n",
            "'/tmp/test-mlm/checkpoint-2000/rng_state.pth' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2000/rng_state.pth'\n",
            "'/tmp/test-mlm/checkpoint-2500' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500'\n",
            "'/tmp/test-mlm/checkpoint-2500/mlm' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/mlm'\n",
            "'/tmp/test-mlm/checkpoint-2500/mlm/adapter_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/mlm/adapter_config.json'\n",
            "'/tmp/test-mlm/checkpoint-2500/mlm/pytorch_adapter.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/mlm/pytorch_adapter.bin'\n",
            "'/tmp/test-mlm/checkpoint-2500/mlm/head_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/mlm/head_config.json'\n",
            "'/tmp/test-mlm/checkpoint-2500/mlm/pytorch_model_head.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/mlm/pytorch_model_head.bin'\n",
            "'/tmp/test-mlm/checkpoint-2500/tokenizer_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/tokenizer_config.json'\n",
            "'/tmp/test-mlm/checkpoint-2500/special_tokens_map.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/special_tokens_map.json'\n",
            "'/tmp/test-mlm/checkpoint-2500/vocab.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/vocab.json'\n",
            "'/tmp/test-mlm/checkpoint-2500/merges.txt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/merges.txt'\n",
            "'/tmp/test-mlm/checkpoint-2500/tokenizer.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/tokenizer.json'\n",
            "'/tmp/test-mlm/checkpoint-2500/training_args.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/training_args.bin'\n",
            "'/tmp/test-mlm/checkpoint-2500/optimizer.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/optimizer.pt'\n",
            "'/tmp/test-mlm/checkpoint-2500/scheduler.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/scheduler.pt'\n",
            "'/tmp/test-mlm/checkpoint-2500/trainer_state.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/trainer_state.json'\n",
            "'/tmp/test-mlm/checkpoint-2500/rng_state.pth' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-2500/rng_state.pth'\n",
            "'/tmp/test-mlm/checkpoint-3000' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000'\n",
            "'/tmp/test-mlm/checkpoint-3000/mlm' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/mlm'\n",
            "'/tmp/test-mlm/checkpoint-3000/mlm/adapter_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/mlm/adapter_config.json'\n",
            "'/tmp/test-mlm/checkpoint-3000/mlm/pytorch_adapter.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/mlm/pytorch_adapter.bin'\n",
            "'/tmp/test-mlm/checkpoint-3000/mlm/head_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/mlm/head_config.json'\n",
            "'/tmp/test-mlm/checkpoint-3000/mlm/pytorch_model_head.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/mlm/pytorch_model_head.bin'\n",
            "'/tmp/test-mlm/checkpoint-3000/tokenizer_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/tokenizer_config.json'\n",
            "'/tmp/test-mlm/checkpoint-3000/special_tokens_map.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/special_tokens_map.json'\n",
            "'/tmp/test-mlm/checkpoint-3000/vocab.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/vocab.json'\n",
            "'/tmp/test-mlm/checkpoint-3000/merges.txt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/merges.txt'\n",
            "'/tmp/test-mlm/checkpoint-3000/tokenizer.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/tokenizer.json'\n",
            "'/tmp/test-mlm/checkpoint-3000/training_args.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/training_args.bin'\n",
            "'/tmp/test-mlm/checkpoint-3000/optimizer.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/optimizer.pt'\n",
            "'/tmp/test-mlm/checkpoint-3000/scheduler.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/scheduler.pt'\n",
            "'/tmp/test-mlm/checkpoint-3000/trainer_state.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/trainer_state.json'\n",
            "'/tmp/test-mlm/checkpoint-3000/rng_state.pth' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3000/rng_state.pth'\n",
            "'/tmp/test-mlm/checkpoint-3500' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500'\n",
            "'/tmp/test-mlm/checkpoint-3500/mlm' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/mlm'\n",
            "'/tmp/test-mlm/checkpoint-3500/mlm/adapter_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/mlm/adapter_config.json'\n",
            "'/tmp/test-mlm/checkpoint-3500/mlm/pytorch_adapter.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/mlm/pytorch_adapter.bin'\n",
            "'/tmp/test-mlm/checkpoint-3500/mlm/head_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/mlm/head_config.json'\n",
            "'/tmp/test-mlm/checkpoint-3500/mlm/pytorch_model_head.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/mlm/pytorch_model_head.bin'\n",
            "'/tmp/test-mlm/checkpoint-3500/tokenizer_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/tokenizer_config.json'\n",
            "'/tmp/test-mlm/checkpoint-3500/special_tokens_map.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/special_tokens_map.json'\n",
            "'/tmp/test-mlm/checkpoint-3500/vocab.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/vocab.json'\n",
            "'/tmp/test-mlm/checkpoint-3500/merges.txt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/merges.txt'\n",
            "'/tmp/test-mlm/checkpoint-3500/tokenizer.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/tokenizer.json'\n",
            "'/tmp/test-mlm/checkpoint-3500/training_args.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/training_args.bin'\n",
            "'/tmp/test-mlm/checkpoint-3500/optimizer.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/optimizer.pt'\n",
            "'/tmp/test-mlm/checkpoint-3500/scheduler.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/scheduler.pt'\n",
            "'/tmp/test-mlm/checkpoint-3500/trainer_state.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/trainer_state.json'\n",
            "'/tmp/test-mlm/checkpoint-3500/rng_state.pth' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-3500/rng_state.pth'\n",
            "'/tmp/test-mlm/checkpoint-500' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500'\n",
            "'/tmp/test-mlm/checkpoint-500/mlm' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/mlm'\n",
            "'/tmp/test-mlm/checkpoint-500/mlm/adapter_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/mlm/adapter_config.json'\n",
            "'/tmp/test-mlm/checkpoint-500/mlm/pytorch_adapter.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/mlm/pytorch_adapter.bin'\n",
            "'/tmp/test-mlm/checkpoint-500/mlm/head_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/mlm/head_config.json'\n",
            "'/tmp/test-mlm/checkpoint-500/mlm/pytorch_model_head.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/mlm/pytorch_model_head.bin'\n",
            "'/tmp/test-mlm/checkpoint-500/tokenizer_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/tokenizer_config.json'\n",
            "'/tmp/test-mlm/checkpoint-500/special_tokens_map.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/special_tokens_map.json'\n",
            "'/tmp/test-mlm/checkpoint-500/vocab.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/vocab.json'\n",
            "'/tmp/test-mlm/checkpoint-500/merges.txt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/merges.txt'\n",
            "'/tmp/test-mlm/checkpoint-500/tokenizer.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/tokenizer.json'\n",
            "'/tmp/test-mlm/checkpoint-500/training_args.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/training_args.bin'\n",
            "'/tmp/test-mlm/checkpoint-500/optimizer.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/optimizer.pt'\n",
            "'/tmp/test-mlm/checkpoint-500/scheduler.pt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/scheduler.pt'\n",
            "'/tmp/test-mlm/checkpoint-500/trainer_state.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/trainer_state.json'\n",
            "'/tmp/test-mlm/checkpoint-500/rng_state.pth' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/checkpoint-500/rng_state.pth'\n",
            "'/tmp/test-mlm/eval_results.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/eval_results.json'\n",
            "'/tmp/test-mlm/merges.txt' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/merges.txt'\n",
            "'/tmp/test-mlm/mlm' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/mlm'\n",
            "'/tmp/test-mlm/mlm/adapter_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/mlm/adapter_config.json'\n",
            "'/tmp/test-mlm/mlm/pytorch_adapter.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/mlm/pytorch_adapter.bin'\n",
            "'/tmp/test-mlm/mlm/head_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/mlm/head_config.json'\n",
            "'/tmp/test-mlm/mlm/pytorch_model_head.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/mlm/pytorch_model_head.bin'\n",
            "'/tmp/test-mlm/README.md' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/README.md'\n",
            "'/tmp/test-mlm/special_tokens_map.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/special_tokens_map.json'\n",
            "'/tmp/test-mlm/tokenizer_config.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/tokenizer_config.json'\n",
            "'/tmp/test-mlm/tokenizer.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/tokenizer.json'\n",
            "'/tmp/test-mlm/trainer_state.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/trainer_state.json'\n",
            "'/tmp/test-mlm/training_args.bin' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/training_args.bin'\n",
            "'/tmp/test-mlm/train_results.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/train_results.json'\n",
            "'/tmp/test-mlm/vocab.json' -> '/gdrive/MyDrive/masakkhane/double-bind/dancing-fish-5/vocab.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WF66sMr6YDzc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}